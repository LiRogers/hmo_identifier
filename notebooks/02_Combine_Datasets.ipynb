{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing any feature generation or modelling we need to combine all the datasets we've fetched into one dataset organised by household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to move up to parent directory to import local functions\n",
    "os.chdir(\"..\")\n",
    "\n",
    "from hmo_identifier.process import merge, address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gazatteer\n",
    "\n",
    "The gazatteer is our base data that we will use the merge other datasets onto. This is the gazatteer for Camden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz = pd.read_csv(\"data/raw/local/gazatteer.csv\")\n",
    "gaz.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can join a lot of the datasets spatially, so we need to convert the pandas dataframe to a geopandas (spatial) dataframe so we can spatially join to other\n",
    "datasets.\n",
    "Careful of the coordinate reference system (CRS) - this gazatteer is in EPSG:27700 (British National Grid, Eastings/Northings) but a lot of the other datasets will be in EPSG:4326 (latitude/longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz = gpd.GeoDataFrame(gaz,\n",
    "                       geometry=gpd.points_from_xy(gaz.x_coordinate,\n",
    "                                                   gaz.y_coordinate),\n",
    "                       crs=27700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the gazatteer now has a spatial location associated with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1)\n",
    "ax = gaz.plot(ax=ax, markersize=0.25)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a spatial dataframe we can add some reference geographies. These files includes census ouput areas and wards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_areas = gpd.read_file(\"data/raw/reference/output_areas.shp\")\n",
    "output_areas = output_areas.to_crs(27700)\n",
    "wards = gpd.read_file(\"data/raw/reference/wards.shp\")\n",
    "wards = wards.to_crs(27700)\n",
    "for ref_geo in [output_areas, wards] :\n",
    "     gaz = (gpd.sjoin(gaz, ref_geo, how='left')\n",
    "            .drop(columns='index_right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the ward and census output area for each gazatteer entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining by UPRN\n",
    "\n",
    "The most reliable way to join in additional datasets is by UPRN. Any datasets that could include a UPRN should be linked to the gazatteer in this way. A helper function to do this is included in hmo_identifier.process.merge\n",
    "\n",
    "The UK Buildings data, which contains information on building structure, includes UPRN so we can start by adding this to the gazatteer.\n",
    "\n",
    "The UK Buildings data comes with a \"link file\", which contains the relationships between the identifier used in this dataset (UPN/unique building number) and UPRN. We need to merge this into the UK Buildings data before we can join in to the gazatteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb = pd.read_csv(\"data/raw/local/uk_buildings.csv\")\n",
    "ukb_uprn = pd.read_csv('data/raw/local/uk_buildings_link.csv')\n",
    "ukb = (ukb\n",
    "       .merge(ukb_uprn)\n",
    "       .drop([\"ubn\", \"upn\"], axis = 1)\n",
    "      .drop_duplicates())\n",
    "\n",
    "gaz = merge.by_uprn(gaz, ukb, \"ukb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gazatteer now has additional columns from UK buildings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with UPRN, the merge won't be perfect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz.merge_ukb.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining by Geographic boundaries\n",
    "\n",
    "Data that is aggregated to different geographic levels can be merged onto the gazatteer.\n",
    "\n",
    "Census data is at the output area level, and IMD is at LSOA. We added geographic levels to the gazatteer earlier, and we can use the helper function merge.by_geog to add the census and IMD data, we just need to get the geography columns into the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = pd.read_csv(\"data/raw/open/census.csv\")\n",
    "census = (census\n",
    "          .drop(columns=['geography', 'date'])\n",
    "          .rename(columns={'geography_code': 'oacd'}))\n",
    "gaz = merge.by_geog(gaz, census, name=\"census\")\n",
    "imd = pd.read_csv('data/raw/open/imd.csv')\n",
    "imd = imd.drop(columns=['ladcd', 'ladnm', 'lsoanm'])\n",
    "gaz = merge.by_geog(gaz, imd, name='imd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have a full match for these matches as they are location based.\n",
    "TODO: Sort out boundary cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz.merge_imd.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1)\n",
    "ax = gaz.plot(ax=ax, markersize=0.25, column=\"imd_decile_imd\")\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining spatial point data\n",
    "\n",
    "Some datasets contain data on a specific location, but not necessarily a specific household. For example, the crime data contains a point location for the crime reported, but this could be on a street or in a park. To join this into our gazatteer we can summarise the data within the area around the household.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime = pd.read_csv(\"data/raw/open/crime.csv\")\n",
    "crime = gpd.GeoDataFrame(crime, geometry = gpd.points_from_xy(crime.location_longitude, crime.location_latitude),\n",
    "                         crs = 4326)\n",
    "crime = crime.to_crs(27700)\n",
    "crime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create indicators for different types of crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime = (crime\n",
    "         .assign(asb = crime.category.isin(['anti-social-behaviour', 'public-order']),\n",
    "                 violent = crime.category.isin(['violent-crime', 'possession-of-weapons']),\n",
    "                 theft = crime.category.isin(['other-theft', 'theft-from-the-person',\n",
    "                                              'burglary', 'robbery',\n",
    "                                              'shoplifting', 'bicycle-theft']),\n",
    "                 other = crime.category.isin(['vehicle-crime', 'drugs',\n",
    "                                              'criminal-damage-arson', 'other-crime'])))\n",
    "crime = crime[[\"asb\", \"violent\", \"theft\", \"other\", \"geometry\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge.by_buffer will summarise the new data in an area around each household (size specified by buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz = merge.by_buffer(ref = gaz, add = crime, name = \"crime\", buffer = 200,\n",
    "                           sum_cols = [\"sum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an indicator of the previlence of different crime types around households"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1)\n",
    "ax = gaz.plot(ax=ax, markersize=0.25, column='asb_sum_crime')\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can apply the same method to the airbnb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb = pd.read_csv('data/raw/open/airbnb.csv')\n",
    "airbnb = gpd.GeoDataFrame(airbnb,\n",
    "                          geometry=gpd.points_from_xy(airbnb.longitude,\n",
    "                                                      airbnb.latitude),\n",
    "                          crs=4326)\n",
    "airbnb = (airbnb\n",
    "          .to_crs(27700)\n",
    "          .assign(price_pp=(airbnb.price\n",
    "                            .str.replace(\"^\\$|\\.00$|,\", \"\")\n",
    "                            .astype(int)\n",
    "                            / airbnb.accommodates))\n",
    "          [['price_pp', 'accommodates', 'geometry']])\n",
    "gaz = merge.by_buffer(ref = gaz, add = airbnb, name = \"abnb\", buffer = 200,\n",
    "                           sum_cols = ['median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1)\n",
    "ax = gaz.plot(ax=ax, markersize=0.25, column='price_pp_median_abnb')\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1)\n",
    "ax = gaz.plot(ax=ax, markersize=0.25, column='accommodates_median_abnb')\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Address Matching\n",
    "\n",
    "Some datasets are at the household level, but don't contain a UPRN. To join these datasets to the gazatteer we need to address match them.\n",
    "\n",
    "As part of the initial data processing, a version of the gazatteer was produced with the 2 different forms of address it incudes, geographical address and delivery point address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz_add = pd.read_csv(\"data/interim/gazatteer_address.csv\")\n",
    "gaz_add.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz_add = gaz_add.loc[gaz_add.uprn.isin(gaz.uprn),:]\n",
    "gaz = pd.merge(gaz, gaz_add[['uprn', 'geo_address','postcode']], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epc = pd.read_csv(\"data/raw/open/epc.csv\", usecols = ['brn', 'address', 'postcode', 'lodgement_date'])\n",
    "epc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EPC data contains a row for each time a property was given an EPC, so there could be multiple for each property. We only need to address match once per property, so lets use the most recent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epc = (epc\n",
    "       .sort_values(by='lodgement_date', ascending=False)\n",
    "       .drop(columns='lodgement_date')\n",
    "       .drop_duplicates('brn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epc = address.match_prep(epc, add_var = 'address')\n",
    "epc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz_geo = (address.match_prep(gaz_add, add_var='geo_address')\n",
    "          .drop(columns='dp_address')\n",
    "          .rename(columns={'clean_address': 'clean_address_geo',\n",
    "                          'numbers': 'numbers_geo',\n",
    "                          'clean_address_flat': 'clean_address_flat_geo'}))\n",
    "gaz_dp = (address.match_prep(gaz_add.loc[~pd.isna(gaz_add.dp_address), :],\n",
    "                             add_var='dp_address')\n",
    "         .drop(columns='geo_address')\n",
    "         .rename(columns={'clean_address': 'clean_address_dp',\n",
    "                          'numbers': 'numbers_dp',\n",
    "                          'clean_address_flat': 'clean_address_flat_dp'}))\n",
    "gaz_add = pd.merge(gaz_geo, gaz_dp, how=\"outer\", on=['uprn', 'postcode']).fillna(\"\")\n",
    "gaz_add.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_matches = address.candidate_matches(ref=gaz_add, ref_id='uprn',\n",
    "                                             ref_addresses=['clean_address_geo', 'clean_address_dp'],\n",
    "                                            add=epc, add_id='brn', add_addresses=['clean_address'])\n",
    "possible_matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_matches.sort_values(by=['numbers_geo_numbers_match', 'clean_address_geo_clean_address_match'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = (possible_matches.loc[(possible_matches.numbers_geo_numbers_match == 1) &\n",
    "                                (possible_matches.clean_address_geo_clean_address_match > 0.7),:]\n",
    "           .sort_values(by=['numbers_geo_numbers_match', 'clean_address_geo_clean_address_match'], ascending=False)\n",
    "          .drop_duplicates('uprn')\n",
    "          .drop_duplicates('brn'))\n",
    "matches.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = (possible_matches.loc[(possible_matches.numbers_geo_numbers_match == 1) &\n",
    "                                (possible_matches.clean_address_geo_clean_address_match > 0.703),:]\n",
    "           .sort_values(by=['numbers_geo_numbers_match', 'clean_address_geo_clean_address_match'], ascending=False)\n",
    "          .drop_duplicates('uprn')\n",
    "          .drop_duplicates('brn'))\n",
    "matches.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = matches.copy()\n",
    "possible_matches = possible_matches.loc[~possible_matches.uprn.isin(matched.uprn),:]\n",
    "possible_matches = possible_matches.loc[~possible_matches.brn.isin(matched.brn),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_matches.sort_values(by=['numbers_dp_numbers_match', 'clean_address_dp_clean_address_match'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = (possible_matches.loc[(possible_matches.numbers_geo_numbers_match == 1) &\n",
    "                                (possible_matches.clean_address_geo_clean_address_match > 0.58) &\n",
    "                                (possible_matches.clean_address_dp_clean_address_match > 0.7),:]\n",
    "           .sort_values(by=['numbers_geo_numbers_match', 'clean_address_geo_clean_address_match'], ascending=False)\n",
    "          .drop_duplicates('uprn')\n",
    "          .drop_duplicates('brn'))\n",
    "matches.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = pd.concat([matched, matches])\n",
    "possible_matches = possible_matches.loc[~possible_matches.uprn.isin(matched.uprn),:]\n",
    "possible_matches = possible_matches.loc[~possible_matches.brn.isin(matched.brn),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_matches.sort_values(by=['numbers_dp_numbers_match', 'clean_address_dp_clean_address_match'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uprn_brn_lookup = matched[['uprn', 'brn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epc = pd.read_csv(\"data/raw/open/epc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz = pd.merge(gaz, uprn_brn_lookup, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_epc = (epc.copy()\n",
    "       .sort_values(by='lodgement_date', ascending=False)\n",
    "       .drop_duplicates('brn')\n",
    "       .drop(columns=['lmk_key', 'address1', 'address2', 'address3', 'postcode',\n",
    "                   'inspection_date', 'local_authority', 'constituency',\n",
    "                    'county', 'mechanical_ventilation', 'address',\n",
    "                    'local_authority_label', 'constituency_label']))\n",
    "latest_epc.columns = latest_epc.columns + \"_epc\"\n",
    "latest_epc = latest_epc.rename(columns={'brn_epc': 'brn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz = pd.merge(gaz, latest_epc, how='left', on='brn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1)\n",
    "ax = gaz.plot(ax=ax, markersize=0.25, column='total_floor_area_epc')\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epc_summary = (epc.copy()\n",
    " .groupby('brn').lmk_key\n",
    " .count()\n",
    ".reset_index().\n",
    "rename(columns={'lmk_key': 'no_entries_epc'}))\n",
    "epc_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz = pd.merge(gaz, epc_summary, how='left', on='brn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can go through the same process with the Land Registry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = pd.read_csv(\"data/raw/open/land_registry.csv\",\n",
    "                 usecols=['trans_id','paon', 'saon', 'street', 'postcode', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = lr.fillna(\"\")\n",
    "lr['address'] = lr[['saon', 'paon', 'street']].apply(lambda x: \" \".join(x), axis = 1)\n",
    "lr = address.match_prep(lr, add_var='address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = (lr\n",
    "      .sort_values('date')\n",
    "      .drop_duplicates('clean_address')\n",
    "     .drop(columns='date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_matches = address.candidate_matches(ref=gaz_add, ref_id='uprn',\n",
    "                                             ref_addresses=['clean_address_geo', 'clean_address_dp'],\n",
    "                                            add=lr, add_id='trans_id', add_addresses=['clean_address'])\n",
    "possible_matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_matches.sort_values(by=['numbers_geo_numbers_match', 'clean_address_geo_clean_address_match'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = (possible_matches.loc[(possible_matches.numbers_geo_numbers_match == 1) &\n",
    "                                (possible_matches.clean_address_geo_clean_address_match > 0.703),:]\n",
    "           .sort_values(by=['numbers_geo_numbers_match', 'clean_address_geo_clean_address_match'], ascending=False)\n",
    "          .drop_duplicates('uprn')\n",
    "          .drop_duplicates('trans_id'))\n",
    "matches.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = matches.copy()\n",
    "possible_matches = possible_matches.loc[~possible_matches.uprn.isin(matched.uprn),:]\n",
    "possible_matches = possible_matches.loc[~possible_matches.trans_id.isin(matched.trans_id),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = (possible_matches\n",
    "           .sort_values(by=['numbers_geo_numbers_match', 'clean_address_geo_clean_address_match'], ascending=False)\n",
    "          .drop_duplicates('uprn')\n",
    "          .drop_duplicates('trans_id'))\n",
    "matches.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uprn_transid_lookup = matched[['uprn', 'trans_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = pd.read_csv(\"data/raw/open/land_registry.csv\",\n",
    "                 usecols = ['trans_id', 'price', 'date', 'prop_type', 'new_build', 'tenure_duration', 'ppd_cat', 'status'])\n",
    "lr.columns = lr.columns + \"_lr\"\n",
    "lr = lr.rename(columns={'trans_id_lr': 'trans_id'})\n",
    "gaz = pd.merge(gaz, uprn_transid_lookup, how = 'left')\n",
    "gaz = pd.merge(gaz, lr, how='left', on=['trans_id'])\n",
    "gaz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1)\n",
    "ax = gaz.plot(ax=ax, markersize=0.25, column='price_lr')\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More approximate address matching\n",
    "\n",
    "Some datasets may not contain the exact address. We therefore need to do a rougher adddress match to add in these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_housing = pd.read_csv(\"data/raw/local/social_housing.csv\")\n",
    "social_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_housing = social_housing[['estate_name', 'ward_name']].drop_duplicates().dropna()\n",
    "#social_housing = social_housing.loc[social_housing.ward_name.str.contains(\"gospel oak|haverstock\", case=False), :]\n",
    "social_housing = social_housing.loc[social_housing.estate_name != \"-\", :]\n",
    "social_housing.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz_add_ward = pd.merge(gaz_add[['uprn', 'clean_address_geo', 'clean_address_dp']], gaz[['uprn', 'wardnm']])\n",
    "gaz_add_ward.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_housing = social_housing.assign(wardnm = social_housing.ward_name.str.replace(\" Ward\", \"\"),\n",
    "                                      clean_estate_name = address.clean_estate_name(social_housing.estate_name))\n",
    "social_housing.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_housing.clean_estate_name = social_housing.clean_estate_name.str.lower()\n",
    "gaz_add_ward['geo_address_list'] = gaz_add_ward.clean_address_geo.str.split()\n",
    "gaz_add_ward['dp_address_list'] = gaz_add_ward.clean_address_dp.str.split()\n",
    "social_housing['estate_list'] = social_housing.clean_estate_name.str.split()\n",
    "gaz_estates = pd.merge(gaz_add_ward, social_housing, how = 'inner')\n",
    "gaz_estates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz_estates[\"estate_in_geo\"] = gaz_estates.apply(lambda x: all(e in x.geo_address_list for e in x.estate_list), axis=1)\n",
    "gaz_estates[\"estate_in_dp\"] = gaz_estates.apply(lambda x: all(e in x.dp_address_list for e in x.estate_list), axis=1)\n",
    "gaz_estates.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz_estates = (gaz_estates.loc[(gaz_estates.estate_in_geo) | (gaz_estates.estate_in_dp), \"uprn\"]\n",
    "               .drop_duplicates()\n",
    "               .tolist())\n",
    "\n",
    "gaz['social_housing'] = gaz.uprn.isin(gaz_estates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMO Register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmos = pd.read_csv(\"data/raw/local/hmo_register.csv\", usecols=['licence_number', 'property_address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmos['address'] = hmos.property_address.str.replace(\"Greater London\", \"\").str.replace(\"\\\\bLondon\\\\b \", \"\").str.replace(\"[A-Z]{1,2}[0-9][A-Z0-9]? ?[0-9][A-Z]{2}\", \"\")\n",
    "hmos['postcode'] = hmos.property_address.str.extract(\"([A-Z]{1,2}[0-9][A-Z0-9]? ?[0-9][A-Z]{2})\", expand=False)\n",
    "hmos = hmos.drop(columns='property_address').drop_duplicates().reset_index(drop=True)\n",
    "hmos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmos = address.match_prep(hmos, add_var = 'address')\n",
    "hmos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_matches = address.candidate_matches(ref=gaz_add, ref_id='uprn',\n",
    "                                             ref_addresses=['clean_address_geo', 'clean_address_dp'],\n",
    "                                            add=hmos, add_id='licence_number', add_addresses=['clean_address'])\n",
    "possible_matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_matches.sort_values(by=['numbers_geo_numbers_match', 'clean_address_geo_clean_address_match'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not going to drop licence_number duplicates as several flats can be covered by one HMO licence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = (possible_matches.loc[(possible_matches.numbers_geo_numbers_match == 1),:]\n",
    "           .sort_values(by=['numbers_geo_numbers_match', 'clean_address_geo_clean_address_match'], ascending=False)\n",
    "          .drop_duplicates('uprn'))\n",
    "matches.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = matches.copy()\n",
    "possible_matches = possible_matches.loc[~possible_matches.uprn.isin(matched.uprn),:]\n",
    "possible_matches = possible_matches.loc[~possible_matches.licence_number.isin(matched.licence_number),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = possible_matches[possible_matches.apply(lambda row: row.clean_address in row.clean_address_geo, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = pd.concat([matched, matches])\n",
    "possible_matches = possible_matches.loc[~possible_matches.uprn.isin(matched.uprn),:]\n",
    "possible_matches = possible_matches.loc[~possible_matches.licence_number.isin(matched.licence_number),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz['hmo'] = gaz.uprn.isin(matched.uprn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz.drop(columns='geometry').to_csv(\"data/interim/gazatteer_combined.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-hmo_test] *",
   "language": "python",
   "name": "conda-env-.conda-hmo_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
